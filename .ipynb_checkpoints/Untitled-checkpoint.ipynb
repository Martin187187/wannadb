{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9852147-43c4-4b94-aa74-8ddf043fbeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\wannadb\\lib\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from wannadb.data.data import Document, DocumentBase\n",
    "from wannadb import resources\n",
    "from wannadb.preprocessing.embedding import SBERTTextEmbedder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6f166b-ed4b-4a9e-890c-78fce5bcbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest.forest_extractor import synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb640319-3bb4-4c98-8ae7-847904113b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_exs = [\"Qatar Airways\", \"Bamboo Airways\"]\n",
    "invalid_exs = [\"Boeing Manufactoring\"]\n",
    "\n",
    "synthesize(valid_exs, invalid_exs, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc6ed24-a65a-4aa5-84a5-e8880e14ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_exs = [\"Lufthansa\", \"Qatar Airways\", \"Bamboo Airways\", \"Unknown Airline\", \"Galatic Airways\",\n",
    "             \"Howto Airways\", \"NoReal Airline\"]\n",
    "invalid_exs = [\"Boeing\", \"Oh well\", \"Who is this\", \"John Smith\", \"Max Mustermann has down\"]\n",
    "\n",
    "synthesize(valid_exs, invalid_exs, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb74e66f-4ac9-4b2c-8d21-53fb0fccf286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1), ('What', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = [(\"Hello\", 1), (\"What\", 2), (\"Is\", 0.5), (\"Poppin\", 0.75)]\n",
    "sorted_ls = sorted(ls, key=lambda x: x[1])\n",
    "sorted_ls[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ae5947-8518-48d2-bba2-27ac0577e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is \"//2442\" valid? (y/n)\n",
      " n\n",
      "Is \"3//8842\" valid? (y/n)\n",
      " n\n",
      "Is \"/22/\" valid? (y/n)\n",
      " n\n",
      "Is \"0/04/\" valid? (y/n)\n",
      " n\n",
      "Is \"28/22/3\" valid? (y/n)\n",
      " n\n",
      "Is \"22/222/8024\" valid? (y/n)\n",
      " n\n",
      "Is \"90//8229\" valid? (y/n)\n",
      " n\n",
      "Is \"42/22/20\" valid? (y/n)\n",
      " n\n",
      "Is \"01/0/0802\" valid? (y/n)\n",
      " n\n",
      "Is \"04/24/424\" valid? (y/n)\n",
      " n\n",
      "Is \"/22/2234\" valid? (y/n)\n",
      " n\n",
      "Is \"0/42/2462\" valid? (y/n)\n",
      " n\n"
     ]
    }
   ],
   "source": [
    "valid_exs = [\"19/08/1996\",\"26/10/1998\",\"22/09/2000\",\"01/12/2001\",\n",
    "             \"29/09/2003\",\"31/08/2015\"]\n",
    "invalid_exs = [\"19/08/96\",\"26-10-1998\",\"22.09.2000\",\"1/12/2001\",\n",
    "               \"29/9/2003\",\"2015/08/31\"]\n",
    "# conditional_invalid_exs = [\"32/08/1996\",\"00/10/1998\"]\n",
    "conditional_invalid_exs = []\n",
    "\n",
    "a = synthesize(valid_exs, [\"19/8/203\"], conditional_invalid_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b34c443-62f2-484b-8c36-d03357e0b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\7.Semester\\DASP\\data\\test.bson\"\n",
    "with open(PATH, \"rb\") as file:\n",
    "    document_base = DocumentBase.from_bson(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd69401-6b45-4d37-b4ac-7c379615e1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SBERTTextEmbedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mResourceManager()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSBERTTextEmbedder\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSBERTBertLargeNliMeanTokensResource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SBERTTextEmbedder' is not defined"
     ]
    }
   ],
   "source": [
    "a = resources.ResourceManager()\n",
    "SBERTTextEmbedder(\"SBERTBertLargeNliMeanTokensResource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bef465df-71b0-40e1-8422-31a1167dbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = document_base.documents[0]\n",
    "doc_text = test_doc.text.lower()\n",
    "\n",
    "# Custom match embedding\n",
    "custom_match = \"samsung phones\"\n",
    "custom_match_embed = resources.MANAGER[\"SBERTBertLargeNliMeanTokensResource\"].encode(custom_match, show_progress_bar=False)\n",
    "\n",
    "# Create ngrams of the document text according to the length of the custom match\n",
    "ngram_length = len(custom_match.split(\" \"))\n",
    "ngrams_doc = ngrams(doc_text.split(), ngram_length)\n",
    "\n",
    "# Create datastructure of ngrams with according positions\n",
    "ngrams_data = [\" \".join(ng) for ng in ngrams_doc]\n",
    "\n",
    "# Calculate the corresponding end position by taking the start location of the ngram, adding the length of all tokens belonging to this\n",
    "# ngram, and account for the spaces as well (ngram_length - 1 many spaces). At the end of the loop, make sure that the location is increased\n",
    "# to refer to the beginning of the next token in the text.\n",
    "# for ng in ngrams_doc:\n",
    "    # end_loc = loc + np.sum([len(ng[x]) for x in range(ngram_length)]) + ngram_length - 1\n",
    "    # ngrams_data.append((\" \".join(ng), loc, end_loc))\n",
    "    # loc += len(ng[0]) + 1\n",
    "    \n",
    "# With statement not necessary inside actual code, just for here\n",
    "# Get embeddings of each ngram with desired embedding model, one could also combine signals here\n",
    "embeddings = resources.MANAGER[\"SBERTBertLargeNliMeanTokensResource\"].encode(ngrams_data, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db3c1f62-3848-4f26-8cbe-3ef4e36421d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crew on a sri lankan airlines plane carrying 202 passengers have extinguished a mid-flight fire triggered by a mobile phone battery in an overhead locker, the carrier said monday.the airline said a ?major? incident was averted by the quick-thinking attendants on the flight sunday from kochi to colombo.smoke was detected shortly after a meal service on the 70-minute flight, it said.the smoke came from an overhead bin, the airline added, thanking its crew for ?averting a major incident?.  crew suspected a lithium battery fire and put the luggage in water after failing to stop the smoke with a fire extinguisher, an airline statement said.?the situation was successfully contained and the bag ceased to emit smoke,? the statement said. ?upon investigation, the crew found a lithium battery pack and two mobile phones in the bag.?the airline did not give the make or model of the battery and the phones involved, but said an investigation was underway into the incident on the airbus a330-200 aircraft.no one was hurt. in october, the carrier joined other airlines in banning samsung note 7 phones from its flights fearing spontaneous combustion.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0678aef3-dfc5-46ba-837e-23111d6e5e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "a mobile\n",
      "111\n",
      "mobile phone\n",
      "807\n",
      "mobile phones\n",
      "814\n",
      "phones in\n",
      "895\n",
      "the phones\n",
      "1079\n",
      "Samsung Note\n",
      "1094\n",
      "phones from\n"
     ]
    }
   ],
   "source": [
    "cosine = lambda x,y: np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y));\n",
    "matches = []\n",
    "threshold = 0.6\n",
    "loc = 0\n",
    "\n",
    "for txt, embed_vector in zip(ngrams_data, embeddings):\n",
    "    cos_sim = cosine(embed_vector, custom_match_embed)\n",
    "    if cos_sim >= threshold:\n",
    "        idx = doc_text.find(txt, loc)\n",
    "        if idx > -1:\n",
    "            matches.append((test_doc, idx, idx+len(txt)))\n",
    "            loc = idx\n",
    "        \n",
    "for match in matches:\n",
    "    print(match[1])\n",
    "    print(match[0].text[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44810ab-7092-49a2-9e2b-78d2cdaae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
